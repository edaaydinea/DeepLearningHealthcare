{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Difference Between End-to-End Memory Network and Self-Attention/Transformer\n",
    "- **End-to-End Memory Network**: Uses external memory that stores key-value pairs, where memory operations are performed to retrieve relevant information. The process is sequential, often using soft attention to select memory cells.\n",
    "  \n",
    "- **Self-Attention/Transformer**: Operates without external memory and directly computes attention across all input tokens in parallel. The transformer model uses self-attention to weigh relationships between tokens, making it more efficient for parallel processing.\n",
    "\n",
    "### 2. Most Innovative Step in the Original Memory Network Model and Why\n",
    "- The **introduction of external memory** that stores relevant facts or key-value pairs and the **use of attention mechanisms** to read/write from memory. This allows the model to perform reasoning tasks by selecting and attending to specific pieces of stored information in memory, a departure from traditional neural networks.\n",
    "\n",
    "### 3. What is NOT True About End-to-End Memory Networks (Fig. 11.9)?\n",
    "- **(b) The size of the embeddings in keys, values, and query are all the same** ❌  \n",
    "  In practice, the embeddings for keys, values, and queries can have different sizes, depending on the design of the memory network.\n",
    "\n",
    "### 4. What is NOT True About Self-Attention?\n",
    "- **(c) The temporal dependency in the input sequences is essential in self-attention models** ❌  \n",
    "  Self-attention models like the Transformer do not explicitly rely on temporal dependencies. Positional encodings are added to account for sequence order, allowing the model to handle inputs without assuming a fixed temporal order.\n",
    "\n",
    "### 5. Method Ideas Used in Transformer Encoder (multiple correct choices):\n",
    "- **(a) Self-attention** ✔️\n",
    "- **(b) Position encoding** ✔️\n",
    "- **(c) Multi-head attention** ✔️\n",
    "- **(d) Residual connection** ✔️\n",
    "\n",
    "### 6. Method Ideas Used in Transformer Decoder (multiple correct choices):\n",
    "- **(a) Masked attention** ✔️ (prevents the model from seeing future tokens in sequence generation)\n",
    "- **(b) Self-attention** ✔️\n",
    "- **(c) Residual connection** ✔️\n",
    "- **(d) Recurrent neural networks** ❌ (Transformers do not use RNNs; they rely solely on attention mechanisms).\n",
    "\n",
    "### 7. What is NOT True About BERT Model?\n",
    "- **(d) BERT provides static embeddings for all words** ❌  \n",
    "  BERT provides **contextual embeddings**, meaning that the same word can have different embeddings depending on its context in the sentence.\n",
    "\n",
    "### 8. Healthcare Application in Doctor2Vec\n",
    "- **Doctor2Vec** is used for **clinical decision support**, learning patient representations based on sequential medical visits to predict future diagnoses and treatments.\n",
    "\n",
    "### 9. Healthcare Application in GameNet\n",
    "- **GameNet** is used for **personalized treatment recommendations** in healthcare by formulating treatment decisions as a game between the patient's health state and treatment outcomes.\n",
    "\n",
    "### 10. Method Ideas Behind G-BERT (multiple correct choices):\n",
    "- **(a) Medical ontology is used to embed medical concepts** ✔️\n",
    "- **(b) BERT model can be used to pre-train on a large number of clinical visits** ✔️\n",
    "- **(c) Temporal dependencies across visits are important in prediction** ✔️\n",
    "- **(d) Multi-task learning is useful in clinical applications** ✔️\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
