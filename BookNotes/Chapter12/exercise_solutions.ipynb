{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the main difference between GAN and VAE?\n",
    "- **GAN (Generative Adversarial Network)**: Uses two networks, a generator and a discriminator, where the generator tries to create synthetic data, and the discriminator tries to differentiate between real and generated data. The training is adversarial.\n",
    "  \n",
    "- **VAE (Variational Autoencoder)**: A generative model that learns a latent representation of the data using probabilistic inference and reconstructs the input data through a decoder. VAE has a probabilistic foundation and provides explicit control over the latent space.\n",
    "\n",
    "### 2. Which of the following is NOT true about GAN method?\n",
    "- **(b) The convergence of GAN model can be easily checked based on the loss function.** ❌  \n",
    "  This statement is incorrect. GANs are notoriously difficult to train, and the loss function does not always provide a reliable signal of convergence.\n",
    "\n",
    "### 3. Which part of the loss function is the generator’s loss?\n",
    "- **(b) Second term**  \n",
    "  The generator's loss is given by the second term, $ Ez∼pz(z)[log(1 − D(G(z)))] $, where it tries to fool the discriminator by making $ D(G(z)) $ close to 1.\n",
    "\n",
    "### 4. Which part of the loss function in the previous question is the generator’s loss?\n",
    "- **(b) Second term**  \n",
    "  The generator focuses on the second term, trying to minimize the difference between real and generated samples.\n",
    "\n",
    "### 5. In MedGAN described in Chap. 12.3, what other neural network models are used besides GAN?\n",
    "- **(d) Autoencoder**  \n",
    "  MedGAN incorporates both GAN and an autoencoder architecture to generate synthetic medical records by modeling complex data distributions.\n",
    "\n",
    "### 6. What exactly are the medical records generated by MedGAN?\n",
    "- **(b) Multi-hot disease category records**  \n",
    "  MedGAN is designed to generate synthetic binary multi-hot representations of disease categories, which are useful for preserving the statistical properties of real patient records.\n",
    "\n",
    "### 7. Which of the following is NOT true about VAE model?\n",
    "- **(c) The loss function in VAE is the exactly same as the loss function in standard autoencoder.** ❌  \n",
    "  The loss function in a VAE combines reconstruction loss with a regularization term (KL divergence), which differs from a standard autoencoder.\n",
    "\n",
    "### 8. Which of the following is NOT true about VAE model?\n",
    "- **(a) The encoder learns qθ (z|x) ∼ N(μ(x), σ(x))** ❌  \n",
    "  The correct form would be $ qθ(z|x) ∼ N(μ(x), σ^2(x)) $. The encoder approximates the posterior distribution of the latent variables.\n",
    "\n",
    "### 9. In the probabilistic view of VAE, which expression corresponds to evidence lower bound (ELBO)? (Multiple correct choices)\n",
    "- **(b) $Eqθ(z|x)[log \\frac{pφ(x,z)}{qθ(z|x)}] $ ✔️**\n",
    "- **(c) $Eqθ(z|x)[log pφ(x|z)] - DKL(qθ(z|x) || pφ(z))$ ✔️**  \n",
    "  These two expressions correspond to the evidence lower bound (ELBO), representing the optimization objective of VAEs.\n",
    "\n",
    "### 10. What is the input to VAE model for drug molecule generation application described in Chap. 12.4? What baseline method do they compare against VAE model?\n",
    "- The **input** to the VAE model for drug molecule generation is the **SMILES (Simplified Molecular Input Line Entry System)** representation of molecules, which encodes molecular structures.\n",
    "  \n",
    "- The **baseline method** they compare against the VAE model is typically **Junction Tree Variational Autoencoder (JT-VAE)**, which also focuses on generating drug-like molecules with valid chemical structures.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
