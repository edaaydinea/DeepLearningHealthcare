{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Types of Health Data That Can Benefit Most from DNN Methods\n",
    "- **Medical Imaging:** DNNs excel at image recognition tasks, making them ideal for analyzing radiology images (X-rays, MRIs, CT scans). They can automatically learn complex features from images, improving diagnostic accuracy.\n",
    "- **Genomic Data:** DNNs can model the intricate relationships in genomic sequences and understand high-dimensional data, making them suitable for predicting disease risk based on genetic information.\n",
    "- **Electronic Health Records (EHR):** DNNs can analyze unstructured data in EHRs, including clinical notes and free-text data, helping identify patterns related to patient outcomes.\n",
    "- **Time-Series Data:** Continuous monitoring data from wearable devices or physiological measurements can benefit from DNNs, particularly recurrent neural networks (RNNs) or long short-term memory (LSTM) networks, which are designed for sequential data.\n",
    "\n",
    "### 2. Types of Health Data That Can Benefit Least from DNN Methods\n",
    "- **Structured Data with Few Features:** Traditional tabular data with a limited number of features may not benefit significantly from DNNs, as simpler models (like logistic regression) can perform adequately without the overhead of complex architectures.\n",
    "- **Small Datasets:** DNNs typically require large amounts of data to generalize effectively. For small datasets, they risk overfitting, making simpler models a better choice.\n",
    "- **Highly Interpretable Data:** In scenarios where model interpretability is crucial (e.g., certain clinical decision-making contexts), simpler models might be preferred over DNNs due to their \"black box\" nature.\n",
    "\n",
    "### 3. Which of the Following is NOT True About Activation Functions?\n",
    "(c) **Activation functions are learned directly from the data by neural network models.**\n",
    "- This statement is NOT true because activation functions are predefined mathematical functions that apply a non-linear transformation to the inputs of a neuron, rather than being learned directly from the data.\n",
    "\n",
    "### 4. What is NOT True About Gradient Descent?\n",
    "(c) **Gradient descent is a specific design method for neural network optimization.**\n",
    "- This statement is NOT true because gradient descent is a general optimization method used for minimizing a wide range of functions, not limited to neural networks.\n",
    "\n",
    "### 5. In Forward Computation, What is the Weight $W^{(1)}_{12}$ Used For?\n",
    "(b) **Connect neuron $x_2$ from the input layer to output neuron $h_1$ in the second layer.**\n",
    "- In this context, $W^{(1)}_{12}$ typically denotes the weight connecting the second input neuron $x_2$ to the first neuron $h_1$ in the hidden layer.\n",
    "\n",
    "### 6. In the General Form of Forward Computation, the Weight Matrix $W^{(l)}$ and Bias Vector $b^{(l)}$ Are Used to Connect?\n",
    "- The weight matrix $W^{(l)}$ and bias vector $b^{(l)}$ connect the neurons from the previous layer (layer $l-1$) to the neurons in the current layer $l$. They define the linear combination of inputs before applying the activation function.\n",
    "\n",
    "### 7. What is True About Backpropagation?\n",
    "(a) **Backpropagation is an efficient way to compute derivatives on parameters in a neural network.**\n",
    "- This statement is true. Backpropagation calculates the gradient of the loss function with respect to each weight by applying the chain rule efficiently.\n",
    "\n",
    "### 8. Which is NOT True About Multilayer Neural Networks?\n",
    "(c) **The linear combination of layer 2 is computed as $z^{(2)}_i = \\sum_j (w^{(1)}_{ji} x_j + b_j)$.**\n",
    "- This statement is NOT true because the notation is incorrect. The linear combination for the second layer should be $z^{(2)}_i = \\sum_j (w^{(2)}_{ji} h_j^{(1)} + b_i^{(2)})$, where $h_j^{(1)}$ are the outputs from the first layer.\n",
    "\n",
    "### 9. Which is NOT True in the Readmission Study Using DNN?\n",
    "(d) **DNN can provide a clear interpretation of its prediction.**\n",
    "- This statement is NOT true. While DNNs can improve prediction accuracy, they are often criticized for being \"black boxes,\" meaning they do not inherently provide clear interpretability of their predictions.\n",
    "\n",
    "### 10. Why Do You Think DNN is a Good Model for QSAR Applications?\n",
    "- **High Dimensionality:** QSAR (Quantitative Structure-Activity Relationship) studies often involve high-dimensional data, where DNNs can effectively learn complex relationships and interactions between features.\n",
    "- **Non-Linear Relationships:** DNNs can model non-linear relationships between chemical structures and biological activity, which is crucial for accurately predicting outcomes in QSAR.\n",
    "- **Feature Learning:** DNNs can automatically extract relevant features from raw data, reducing the need for extensive feature engineering, which can be time-consuming in QSAR modeling.\n",
    "- **Generalization Ability:** When trained on sufficient data, DNNs can generalize well to unseen compounds, improving the predictive power of QSAR models.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
