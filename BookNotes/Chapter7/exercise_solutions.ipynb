{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the main difference between RNN and bidirectional RNN?\n",
    "\n",
    "The main difference is that in a **Bidirectional RNN**, there are two RNNs: one processes the input sequence **forward** (from past to future) and the other processes the sequence **backward** (from future to past). These two hidden states are then **combined**, allowing the model to capture both past and future information in sequences. Regular **RNNs** only capture past information.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What is the main difference between LSTM and GRU?\n",
    "\n",
    "The main difference between **LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)** is in their **gating mechanisms**:\n",
    "- **LSTM** has three gates: the input gate, forget gate, and output gate, which regulate how information flows through the cell.\n",
    "- **GRU** has two gates: the update gate and reset gate, making it a simpler architecture compared to LSTM. GRU merges the forget and input gates into one gate (the update gate), and it tends to be more computationally efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What data are suitable for RNN to model? (Multiple correct choices)\n",
    "The following data types are suitable for RNNs:\n",
    "- **(a) Clinical notes**: Text data with sequential dependencies can be modeled with RNNs.\n",
    "- **(c) Longitudinal medical records**: These involve a sequence of patient visits, making RNNs suitable.\n",
    "- **(d) Time series such as electrocardiogram**: Time-dependent data is ideal for RNNs.\n",
    "\n",
    "**(b) Medical images** are not sequential data and are usually better modeled with Convolutional Neural Networks (CNNs).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Which of the following architectures is best for sequential diagnosis prediction (i.e., predicting the disease diagnosis of the current visit based on a patientâ€™s visit history)?\n",
    "\n",
    "For **sequential diagnosis prediction**, a **Recurrent Neural Network (RNN)** architecture would be best because it is designed for handling sequential data. The most appropriate architecture would be a **long short-term memory (LSTM)** or **Gated Recurrent Unit (GRU)** for better handling of long-term dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. What are the potential issues of backpropagation through time (BPTT) algorithm?\n",
    "\n",
    "Potential issues with **Backpropagation Through Time (BPTT)** include:\n",
    "- **Vanishing and exploding gradients**: Gradients can become very small (vanish) or extremely large (explode) when propagated through many time steps, making learning difficult or unstable.\n",
    "- **Computational cost**: BPTT is computationally expensive because it requires storing all hidden states and gradients for every time step.\n",
    "- **Memory usage**: It requires significant memory to store all the states and weights for backpropagation over long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. In the GRU model, can we replace the tanh activation function with sigmoid activation in Eq. (7.14)? Why?\n",
    "\n",
    "No, you generally should not replace the **tanh** activation function with a **sigmoid** function in GRU because the output range of **tanh** is between **-1 and 1**, which allows the GRU to represent both positive and negative dependencies. **Sigmoid** activation outputs values between **0 and 1**, which would limit the ability of the GRU to model both excitatory and inhibitory effects in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What is NOT true about bidirectional RNNs?\n",
    "\n",
    "**(d) Two RNN models have to be trained separately.**\n",
    "\n",
    "This is **not true** because **bidirectional RNNs** are trained together, not separately. The forward and backward RNNs are trained in parallel to capture both directions of the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. What is NOT true about the Seq2seq model?\n",
    "\n",
    "**(d) Seq2seq model ensures input sequence and output sequence to be the same length.**\n",
    "\n",
    "This is **not true** because the **Seq2Seq model** does not require the input and output sequences to be the same length. For example, in machine translation, the number of words in the source sentence and the translated sentence can differ.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Which of the following is NOT true about RNN applications to healthcare?\n",
    "\n",
    "**(a) We need training data of the same length to train RNN models.**\n",
    "\n",
    "This is **not true** because **RNNs** can handle variable-length sequences, which is one of their key advantages. They can process sequences of different lengths by iterating through each element in the sequence, making them ideal for time-series data or longitudinal records where sequence lengths can vary."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
