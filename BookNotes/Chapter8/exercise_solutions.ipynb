{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Main Differences Between Autoencoder and Principal Component Analysis (PCA)\n",
    "- **Autoencoder**: \n",
    "  - Non-linear dimensionality reduction technique using neural networks.\n",
    "  - Can learn complex, non-linear mappings and representations.\n",
    "  - Typically consists of an encoder, bottleneck (latent space), and decoder.\n",
    "  - Requires backpropagation and gradient-based optimization.\n",
    "  \n",
    "- **Principal Component Analysis (PCA)**: \n",
    "  - Linear dimensionality reduction technique.\n",
    "  - Projects data onto orthogonal principal components (axes with maximum variance).\n",
    "  - Results in a lower-dimensional linear subspace.\n",
    "  - Does not involve neural networks or non-linear transformation.\n",
    "\n",
    "### 2. Main Difference Between Autoencoder and Denoising Autoencoder\n",
    "- **Autoencoder**: \n",
    "  - Learns to reconstruct input data from a compressed latent representation.\n",
    "  \n",
    "- **Denoising Autoencoder**: \n",
    "  - Adds noise to the input data and trains the model to recover the original, clean input.\n",
    "  - Used to improve the robustness of the model against noisy data.\n",
    "\n",
    "### 3. Most Analogous Method to Autoencoders\n",
    "- (b) **Principal Component Analysis (PCA)**: \n",
    "  - Autoencoders are often considered a non-linear generalization of PCA. Both perform dimensionality reduction but autoencoders are more flexible due to their non-linear transformations.\n",
    "\n",
    "### 4. Which of the Following is NOT True About Autoencoders?\n",
    "- (b) **It is a lossless compression technique.**\n",
    "  - Autoencoders perform lossy compression since some information may be lost when encoding the data into a lower-dimensional space.\n",
    "\n",
    "### 5. What is NOT True About Sparse Autoencoder?\n",
    "- (b) **Sigmoid activation function is used to produce latent code h.**\n",
    "  - Sparse autoencoders can use various activation functions (e.g., ReLU), not necessarily sigmoid. The sparsity constraint is introduced by adding a regularization term, which is unrelated to the specific choice of activation function.\n",
    "\n",
    "### 6. What is NOT True About Denoising Autoencoder?\n",
    "- (c) **It is more expensive to train because of random noise added to the original input.**\n",
    "  - Adding random noise to the input doesn't significantly increase the training cost compared to regular autoencoders. It mainly improves robustness.\n",
    "\n",
    "### 7. Parameters Learned First in Stacked Autoencoders\n",
    "- The model first learns the **weights** and **biases** of the individual encoders. The stacked autoencoder is built by training each layer (or autoencoder) sequentially, learning these parameters in a layer-wise manner before stacking them together.\n",
    "\n",
    "### 8. What is NOT True About Stacked Autoencoder?\n",
    "- (c) **It is trained in an end-to-end fashion as a single model using backpropagation.**\n",
    "  - Stacked autoencoders are usually trained **layer by layer** (unsupervised, greedy training), not as a single end-to-end model initially. Once pre-training is done, fine-tuning may use end-to-end backpropagation.\n",
    "\n",
    "### 9. Model Used Before Applying Autoencoder in the Phenotype Discovery Paper\n",
    "- In the phenotype discovery paper [94], **topic modeling** methods like **Latent Dirichlet Allocation (LDA)** or **Non-negative Matrix Factorization (NMF)** are often applied before using autoencoders for phenotype discovery.\n",
    "\n",
    "### 10. Variant of Autoencoder Model Used in Deep Patient Paper [113]\n",
    "- (d) **Sparsity level on each dimension of h needs to be specified.**\n",
    "  - The **Deep Patient** paper uses stacked denoising autoencoders, but with an emphasis on sparse representations. This requires specifying sparsity constraints on each dimension of the latent space `h`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
