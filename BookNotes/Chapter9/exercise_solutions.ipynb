{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Difference Between Sequence-to-Sequence Model with RNN and with Attention\n",
    "- **Sequence-to-Sequence with RNN**: \n",
    "  - The model encodes the input sequence into a fixed-length vector (hidden state) using an RNN (such as LSTM or GRU) and then decodes this vector to generate the output sequence.\n",
    "  - Limitation: Fixed-length vectors struggle to capture long-range dependencies in longer sequences.\n",
    "\n",
    "- **Sequence-to-Sequence with Attention**: \n",
    "  - Attention mechanism allows the decoder to focus on different parts of the input sequence during decoding.\n",
    "  - Instead of encoding the entire input sequence into a single fixed-length vector, the attention mechanism computes a weighted sum of the encoder hidden states (dynamic context vector) at each time step, which helps capture long-range dependencies more effectively.\n",
    "\n",
    "### 2. Direct Input to Compute Attention Weight $\\alpha_{ij}$\n",
    "- The direct input to compute the attention weight $\\alpha_{ij}$ is typically:\n",
    "  - The **hidden state of the decoder** at time step $i$ (denoted $s_i$).\n",
    "  - The **hidden state of the encoder** at time step $j$ (denoted $h_j$).\n",
    "  \n",
    "  These inputs are used to compute the alignment score (e.g., using a dot product, additive function, or other scoring methods), which is then normalized (typically using softmax) to get the attention weights $\\alpha_{ij}$.\n",
    "\n",
    "### 3. How to Compute Dynamic Context Vector $c_i$\n",
    "- The dynamic context vector $c_i$ is computed as the weighted sum of the encoder hidden states:\n",
    "  $$\n",
    "  c_i = \\sum_j \\alpha_{ij} h_j\n",
    "  $$\n",
    "  Where:\n",
    "  - $\\alpha_{ij}$ is the attention weight (the importance of encoder hidden state $h_j$ for decoder step $i$).\n",
    "  - $h_j$ is the encoder hidden state at time step $j$.\n",
    "\n",
    "### 4. Direct Inputs to Decoder $s_i$\n",
    "- The direct inputs to the decoder hidden state $s_i$ are typically:\n",
    "  - The **previous decoder hidden state** $s_{i-1}$.\n",
    "  - The **dynamic context vector** $c_i$ (computed from attention mechanism).\n",
    "  - The **previous decoder output** or the **embedding** of the previously generated word $y_{i-1}$.\n",
    "\n",
    "### 5. Steps to Compute Attention Weights in RETAIN Method (Fig. 9.9)\n",
    "- In the RETAIN method:\n",
    "  - Attention weights are computed using two separate RNNs:\n",
    "    1. **Alpha RNN**: Computes attention weights (importance of each timestep in the input sequence) over the input sequence (visit-level attention).\n",
    "    2. **Beta RNN**: Computes weights for each feature (within a visit) to form a context vector for patient records (feature-level attention).\n",
    "\n",
    "### 6. CAML Method [114]: Other Neural Network Architecture and Clinical Application\n",
    "- **Other Neural Network Architecture**: Convolutional Neural Networks (CNNs) are used in addition to the attention mechanism.\n",
    "- **Clinical Application**: CAML (Convolutional Attention-based Model for Medical Code Prediction) is used for **predicting medical codes** (such as ICD codes) from clinical notes. The CNN captures local text features, and the attention mechanism helps the model focus on important sections of the text for code prediction.\n",
    "\n",
    "### 7. Input Data to MINA Method [73]\n",
    "- The input data to the MINA (Multi-level Attention Networks for Electronic Health Records) method is typically **patient EHR data**, particularly **sequential clinical events** (such as visits, diagnoses, and treatments). The model applies attention at multiple levels (visit-level, feature-level) to capture important patterns in longitudinal patient records."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
