{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Target\n",
    "\n",
    "This lecture introduces the basics of machine learning, focusing on three key topics: supervised learning, unsupervised learning, and evaluation metrics. Let's summarize the content covered in this session:\n",
    "\n",
    "### 1. **Supervised Learning:**\n",
    "   - **Predictive Model Pipeline:** This involves several steps that are repeated iteratively to build a predictive model. The steps include:\n",
    "     1. **Prediction Target Definition:** Identifying the prediction target, which should be both interesting and feasible.\n",
    "     2. **Cohort Construction:** Defining a cohort of relevant patients or subjects.\n",
    "     3. **Feature Construction:** Identifying potentially relevant features from the data.\n",
    "     4. **Feature Selection:** Choosing the most relevant features for predicting the target variable.\n",
    "     5. **Model Computation:** Developing the predictive model (classification or regression).\n",
    "     6. **Model Evaluation:** Assessing model performance and iterating until satisfactory results are achieved.\n",
    "\n",
    "   - **Prediction Target:** \n",
    "     - Targets should be both interesting and feasible. Interesting targets can be determined by consulting domain experts, studying existing research, or applying common sense (e.g., predicting high-cost conditions like heart failure).\n",
    "     - Feasibility can be assessed by comparing to human performance, leveraging experience from similar projects, or reviewing literature.\n",
    "\n",
    "   - **Heart Failure Prediction:**\n",
    "     - Heart failure is highlighted as a significant healthcare problem, with 550,000 new cases each year in the U.S.\n",
    "     - Early detection of heart failure can reduce costs, hospitalization, and mortality, as well as improve the quality of life and influence clinical guidelines.\n",
    "\n",
    "This lecture emphasizes the importance of selecting the right prediction target and iterating on the pipeline to refine and improve machine learning models. The use of domain expertise and prior research is essential to ensure that machine learning efforts are both meaningful and achievable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Construction\n",
    "\n",
    "**Cohort Construction** is a critical step in building predictive models, particularly in healthcare settings. This process involves defining a study population and determining the relevant features for analysis. Here’s a breakdown of the key concepts and approaches:\n",
    "\n",
    "### **1. Importance of Cohort Construction:**\n",
    "   - **Avoiding Obvious Models:** General models, such as predicting mortality based on age, may be too simplistic and less useful. It's important to focus on specific populations to create more meaningful predictions.\n",
    "   - **Target Population:** Identifying a relevant population, such as focusing on heart failure in African Americans, can lead to more actionable insights.\n",
    "   - **Cost Considerations:** Data acquisition can be costly, so constructing a well-defined cohort helps manage expenses.\n",
    "\n",
    "### **2. Study Design Types:**\n",
    "   - **Prospective vs. Retrospective Studies:**\n",
    "     - **Prospective Study:**\n",
    "       - **Process:** Identify the cohort first, then collect data over time.\n",
    "       - **Characteristics:** More controlled, typically more expensive, time-consuming, and often involves smaller datasets.\n",
    "       - **Advantages:** Data is collected specifically for the study, leading to less noise.\n",
    "     - **Retrospective Study:**\n",
    "       - **Process:** Identify the cohort from existing data sources like electronic health records.\n",
    "       - **Characteristics:** Less expensive, faster, can handle larger datasets, but often involves more noise because the data was not originally collected for the specific study.\n",
    "       - **Advantages:** Utilizes already available data, but may include biases or inaccuracies not relevant to the current research.\n",
    "\n",
    "### **3. Cohort Studies:**\n",
    "   - **Cohort Study:**\n",
    "     - **Goal:** Select a group of patients exposed to a specific risk to study outcomes.\n",
    "     - **Example:** Predicting heart failure readmission by including all patients discharged with heart failure.\n",
    "   - **Case-Control Study:**\n",
    "     - **Goal:** Identify patients with (cases) and without (controls) the condition to study differences.\n",
    "     - **Example:** Predicting heart failure by comparing cases (patients with heart failure) and controls (healthy patients).\n",
    "     - **Types of Matching:**\n",
    "       - **Group Matching:** Matching groups on certain statistics rather than individual features.\n",
    "       - **Individual Matching:** Matching individual cases and controls on specific features.\n",
    "       - **Propensity Matching:** Matching based on a single score derived from logistic regression, simplifying the matching process.\n",
    "       - **Nested Case-Control Matching:** Matching each case with multiple controls to create a more robust dataset.\n",
    "\n",
    "### **Summary of Key Points:**\n",
    "1. **Prospective Study:**\n",
    "   - **More Control:** Data collected specifically for the study.\n",
    "   - **Higher Cost and Time:** Expensive and time-consuming but usually less noisy.\n",
    "   - **Smaller Datasets:** Typically involves a smaller sample size.\n",
    "\n",
    "2. **Retrospective Study:**\n",
    "   - **Uses Existing Data:** Data collected for other purposes.\n",
    "   - **Lower Cost and Time:** Faster and cheaper but may be noisier.\n",
    "   - **Larger Datasets:** Often involves a larger sample size.\n",
    "\n",
    "3. **Cohort vs. Case-Control Studies:**\n",
    "   - **Cohort Study:** Follows a group over time to observe outcomes.\n",
    "   - **Case-Control Study:** Compares cases and controls to identify risk factors or predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Construction\n",
    "\n",
    "**Feature construction** is a key step in building predictive models, especially when dealing with longitudinal data such as electronic health records (EHRs). This process involves creating meaningful features from raw data to improve model performance. Here's a detailed breakdown of feature construction and selection:\n",
    "\n",
    "**1. Event Sequences and Windows:**\n",
    "   - **Event Sequences:** Patient data often come as sequences of clinical events (e.g., diagnoses, medications, lab results) recorded over time.\n",
    "   - **Diagnosis Date:** The date on which a diagnosis is made (e.g., heart failure) is crucial. Control patients' diagnosis dates are typically aligned with cases for fair comparison.\n",
    "   - **Observation Window:** The period before the index date used to gather patient data for feature construction.\n",
    "   - **Prediction Window:** The timeframe within which the target outcome is predicted (e.g., heart failure).\n",
    "\n",
    "**2. Constructing Features:**\n",
    "   - **Frequency Counts:** Counting occurrences of events (e.g., number of diabetes diagnoses).\n",
    "   - **Averages:** Calculating average values of repeated measurements (e.g., average A1C levels).\n",
    "   - **Window Lengths:** The length of the observation and prediction windows impacts the model. \n",
    "\n",
    "### **Impact of Window Sizes:**\n",
    "\n",
    "1. **Easiest Timeline to Model:**\n",
    "   - **Answer:** **Large Observation Window, Small Prediction Window** (Option A)\n",
    "     - **Reasoning:** More data (large observation window) allows for better feature construction, and predicting near future (small prediction window) is generally easier.\n",
    "\n",
    "2. **Most Useful Model (Ideal Case):**\n",
    "   - **Answer:** **Small Observation Window, Large Prediction Window** (Option B)\n",
    "     - **Reasoning:** Ideally, predicting far into the future with minimal data would be the most useful, though this is often challenging.\n",
    "\n",
    "3. **Optimal Prediction Window Curve:**\n",
    "   - **Answer:** **B** (Curve showing accuracy that remains high up to a longer prediction window before dropping off)\n",
    "     - **Reasoning:** Indicates that the model maintains high accuracy for longer prediction windows compared to others.\n",
    "\n",
    "4. **Optimal Observation Window:**\n",
    "   - **Answer:** **C (630 days)**\n",
    "     - **Reasoning:** Performance improves with longer observation windows but plateaus after a certain point, indicating diminishing returns beyond 630 days.\n",
    "\n",
    "### **Feature Selection**\n",
    "\n",
    "1. **Purpose:**\n",
    "   - **Goal:** Identify which features are most predictive of the target outcome to include in the final model.\n",
    "   - **Relevance:** Not all features are equally useful for prediction. The feature selection process aims to filter out irrelevant or redundant features.\n",
    "\n",
    "2. **Types of Features:**\n",
    "   - **Demographics:** Age, sex, race.\n",
    "   - **Symptoms:** Observed symptoms related to conditions.\n",
    "   - **Diagnoses:** Historical medical conditions.\n",
    "   - **Medications:** Past and current medications.\n",
    "   - **Lab Results:** Various test results.\n",
    "   - **Vital Signs:** Measurements like blood pressure.\n",
    "\n",
    "3. **Feature Selection Strategies:**\n",
    "   - **Filter Methods:** Statistical tests to select features based on their relationship with the target variable.\n",
    "   - **Wrapper Methods:** Evaluating subsets of features by training and validating models.\n",
    "   - **Embedded Methods:** Feature selection integrated within the model training process.\n",
    "\n",
    "**Example Analysis:**\n",
    "   - **Patient Data:** Features like age, blood pressure, diabetes status, and other health indicators.\n",
    "   - **Feature Relevance:** For predicting heart failure, relevant features might include past diagnoses of heart conditions, medication usage, and lab results related to heart health.\n",
    "\n",
    "### **Summary:**\n",
    "- **Feature Construction** involves creating features from raw data by defining observation and prediction windows.\n",
    "- **Feature Selection** focuses on identifying which features are most relevant for predicting the target outcome from a potentially large set of features.\n",
    "\n",
    "Understanding these processes allows for better model performance by using meaningful data and selecting the most predictive features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model and Evaluation\n",
    "\n",
    "**1. Predictive Models Overview**\n",
    "\n",
    "A predictive model maps features of a patient $x$ to a target outcome $y$. Depending on the nature of $y$, the model is either:\n",
    "\n",
    "- **Regression Model**: When $y$ is continuous (e.g., predicting the cost of a patient visit). Methods include:\n",
    "  - Linear Regression\n",
    "  - Generalized Additive Models\n",
    "\n",
    "- **Classification Model**: When $y$ is categorical (e.g., predicting whether a patient is healthy or not). Methods include:\n",
    "  - Logistic Regression\n",
    "  - Support Vector Machine (SVM)\n",
    "  - Decision Trees\n",
    "  - Random Forests\n",
    "  - Neural Networks\n",
    "\n",
    "**2. Model Evaluation**\n",
    "\n",
    "Evaluating a model is crucial to understanding its performance and generalizability:\n",
    "\n",
    "- **Training Error**: Performance of the model on the data it was trained on. This is not a useful measure because the model might overfit to the training data.\n",
    "  \n",
    "- **Test Error**: Performance of the model on unseen data (test set). This is a better metric for assessing how the model will perform on new data.\n",
    "\n",
    "**3. Cross-Validation Techniques**\n",
    "\n",
    "Cross-validation is a method to estimate the model’s performance by using different subsets of the data:\n",
    "\n",
    "- **Leave-One-Out Cross-Validation (LOOCV)**:\n",
    "  - Each data point is used once as a test set while the rest are used as the training set.\n",
    "  - Repeat for all data points, then average the results.\n",
    "  - **Pros**: Provides a nearly unbiased estimate of model performance.\n",
    "  - **Cons**: Computationally expensive, especially with large datasets.\n",
    "\n",
    "- **K-Fold Cross-Validation**:\n",
    "  - The dataset is divided into $K$ equal-sized folds.\n",
    "  - Each fold is used once as a test set while the remaining $K-1$ folds are used for training.\n",
    "  - Repeat for each fold and average the results.\n",
    "  - **Pros**: More efficient than LOOCV, provides a good estimate of model performance.\n",
    "  - **Cons**: The choice of $K$ can affect performance; typically, $K$ is set to 5 or 10.\n",
    "\n",
    "- **Randomized Cross-Validation**:\n",
    "  - The data is randomly split into training and test sets multiple times.\n",
    "  - The model is trained and tested for each split, then results are averaged.\n",
    "  - **Pros**: Can be more flexible with large datasets and allows for numerous iterations.\n",
    "  - **Cons**: Some data points may not be included in the test set, which could affect the evaluation.\n",
    "\n",
    "**4. Train-Validation-Test Split**\n",
    "\n",
    "For complex models, especially in deep learning:\n",
    "\n",
    "- **Training Set**: Used to train the model.\n",
    "- **Validation Set**: Used to tune model hyperparameters (e.g., number of layers, neurons).\n",
    "- **Test Set**: Used to estimate the final model’s performance.\n",
    "\n",
    "**Steps in Model Building and Evaluation**:\n",
    "\n",
    "1. **Define Prediction Target**: Identify what you want to predict.\n",
    "2. **Cohort Construction**: Select the relevant patients and construct your dataset.\n",
    "3. **Feature Construction**: Create features from the raw data.\n",
    "4. **Feature Selection**: Choose which features are most relevant for predicting the target.\n",
    "5. **Build Predictive Model**: Choose and apply the appropriate algorithm (regression or classification).\n",
    "6. **Evaluate Model**: Use cross-validation and the test set to assess performance.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The predictive modeling pipeline involves defining the target, constructing and selecting features, and building and evaluating the model. Each step requires careful consideration and iteration to improve the model's performance. By understanding and applying cross-validation and test sets properly, you can ensure that your model generalizes well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "### Unsupervised Learning: Dimensionality Reduction and Clustering\n",
    "\n",
    "#### Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction aims to reduce the number of features in your dataset while preserving as much relevant information as possible. This process simplifies models and can improve computational efficiency and model performance. Two key methods for dimensionality reduction are **Singular Value Decomposition (SVD)** and **Principal Component Analysis (PCA)**.\n",
    "\n",
    "#### Singular Value Decomposition (SVD)\n",
    "\n",
    "**SVD** is a matrix factorization technique used to decompose a matrix $X$ into three other matrices:\n",
    "\n",
    "\\[ X = U \\Sigma V^T \\]\n",
    "\n",
    "- **U**: Left singular vectors\n",
    "- **\\(\\Sigma\\)**: Diagonal matrix of singular values\n",
    "- **V^T**: Transpose of the matrix containing right singular vectors\n",
    "\n",
    "**Applications of SVD**:\n",
    "\n",
    "1. **Clustering**: SVD can be used to identify clusters within data. By examining the singular vectors and values, you can separate data into distinct clusters.\n",
    "\n",
    "2. **Document-Term Matrix Example**: In a document-term matrix where rows represent documents and columns represent terms:\n",
    "   - **Red Cluster**: Computer Science-related documents and terms.\n",
    "   - **Purple Cluster**: Medical-related documents and terms.\n",
    "\n",
    "Each rank-1 component derived from SVD corresponds to a different cluster in the data. \n",
    "\n",
    "**Matrix Views**:\n",
    "- **Matrix View**: Direct decomposition of the input matrix $X$.\n",
    "- **Spectral View**: Interpretation of SVD components as rank-1 matrices, each contributing to different clusters.\n",
    "\n",
    "**Curious Question**:\n",
    "Given a document-by-term matrix $A$, what is $A^T A$?\n",
    "- **Answer**: It is a term-to-term similarity matrix. This matrix measures the similarity between terms based on their co-occurrence across documents.\n",
    "\n",
    "#### Principal Component Analysis (PCA)\n",
    "\n",
    "**PCA** is another popular dimensionality reduction technique that uses SVD to transform the data into a lower-dimensional space:\n",
    "\n",
    "1. **SVD and PCA**: PCA can be viewed as a specific application of SVD. In PCA:\n",
    "   - The matrix $X$ is decomposed as $X = U \\Sigma V^T$.\n",
    "   - The principal components are found by multiplying $U$ and $\\Sigma$.\n",
    "\n",
    "2. **Principal Components**:\n",
    "   - The product $U \\Sigma$ provides the principal components, which represent the directions of maximum variance in the data.\n",
    "   - The matrix $V^T$ provides the loadings, which are the directions in the original feature space.\n",
    "\n",
    "**Example**:\n",
    "For a dataset in a two-dimensional space, projecting it onto a one-dimensional space involves:\n",
    "- Identifying the direction of the first principal component (the direction with the maximum variance).\n",
    "- Projecting the data points onto this direction, resulting in a one-dimensional representation.\n",
    "\n",
    "**Steps in PCA**:\n",
    "1. **Compute SVD**: Factorize matrix $X$ into $U$, $\\Sigma$, and $V^T$.\n",
    "2. **Obtain Principal Components**: Multiply $U$ by $\\Sigma$ to get the principal components.\n",
    "3. **Determine Directions**: Use $V^T$ to understand the direction of these components.\n",
    "\n",
    "**Visualization**:\n",
    "- In a 2D space, PCA finds the direction of the largest variance and projects data points onto this line, simplifying the data into one dimension.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Dimensionality reduction is crucial for making complex data more manageable and interpretable. **SVD** provides insights into clustering by factorizing data matrices into components that can reveal underlying structures. **PCA** simplifies data by projecting it into a lower-dimensional space while retaining the most significant variance, facilitating easier analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "### Clustering and K-Means Algorithm\n",
    "\n",
    "#### Clustering Overview\n",
    "\n",
    "**Clustering** is a technique used to group data points into clusters where data points in the same cluster are more similar to each other than to those in other clusters. This can be done on various types of data, such as patients and diseases.\n",
    "\n",
    "- **Patient Clustering**: Group patients based on their disease profiles.\n",
    "- **Disease Clustering**: Group diseases based on patterns observed in patient data.\n",
    "\n",
    "#### K-Means Clustering\n",
    "\n",
    "**K-Means** is one of the most popular clustering algorithms, and it aims to partition data into $K$ clusters. The objective is to minimize the sum of squared distances between data points and their respective cluster centers.\n",
    "\n",
    "**Algorithm Steps**:\n",
    "\n",
    "1. **Initialize**: Choose $K$ initial cluster centers randomly or from the data points.\n",
    "\n",
    "2. **Assign Points**: Assign each data point to the nearest cluster center. This step partitions the data into clusters based on the proximity to the centers.\n",
    "\n",
    "3. **Update Centers**: Recalculate the cluster centers as the mean of all points assigned to each cluster.\n",
    "\n",
    "4. **Convergence Check**: Check if the cluster assignments have changed since the last iteration or if the maximum number of iterations has been reached. If not converged, repeat steps 2 and 3.\n",
    "\n",
    "5. **Output Clusters**: Once converged, the final clusters are output.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "- **Initialization**: Start with random cluster centers.\n",
    "- **Assignment**: Assign data points to the nearest center, forming clusters.\n",
    "- **Update**: Recalculate centers based on the mean of assigned points.\n",
    "- **Iteration**: Repeat the assignment and update steps until the centers stabilize.\n",
    "\n",
    "**Visualization**:\n",
    "\n",
    "In a 2D space, K-means might look like this:\n",
    "\n",
    "1. **Initial Centers**: Place initial centers (e.g., crosses) randomly.\n",
    "2. **Assign Points**: Each point is assigned to the closest center, forming clusters.\n",
    "3. **Update Centers**: Move the centers to the mean of assigned points.\n",
    "4. **Iterate**: Continue until clusters no longer change significantly.\n",
    "\n",
    "**Computational Complexity**:\n",
    "\n",
    "The computational complexity of the K-means algorithm is given by:\n",
    "\n",
    "$$\\text{Complexity} = n \\times k \\times d \\times i$$\n",
    "\n",
    "where:\n",
    "- $n$ = Number of data points\n",
    "- $k$ = Number of clusters\n",
    "- $d$ = Dimensionality of each data point\n",
    "- $i$ = Number of iterations\n",
    "\n",
    "**Explanation**:\n",
    "- For each iteration, each of the $n$ points is compared to $k$ centers, which requires $O(k \\times d)$ operations per point.\n",
    "- This results in a total complexity of $O(n \\times k \\times d)$ per iteration.\n",
    "- With $i$ iterations, the final complexity is $O(n \\times k \\times d \\times i)$.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Clustering** groups data into clusters based on similarity.\n",
    "- **K-Means** clusters data by iteratively assigning points to the nearest center and updating cluster centers until convergence.\n",
    "- The algorithm's complexity is influenced by the number of data points, clusters, dimensions, and iterations, making it computationally intensive but effective for many clustering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "#### Introduction to Evaluation Metrics\n",
    "\n",
    "When assessing the performance of classification models, it's essential to use appropriate metrics to understand how well the model is performing. These metrics are derived from a confusion matrix, which summarizes the outcomes of predictions against the actual ground truth values.\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "For binary classification, the confusion matrix provides four possible outcomes:\n",
    "\n",
    "- **True Positive (TP)**: Correctly predicted positive cases.\n",
    "- **False Positive (FP)**: Incorrectly predicted positive cases (Type I Error).\n",
    "- **False Negative (FN)**: Incorrectly predicted negative cases (Type II Error).\n",
    "- **True Negative (TN)**: Correctly predicted negative cases.\n",
    "\n",
    "Here's a summary of these terms:\n",
    "- **True Positive (TP)**: Prediction and actual value are both positive.\n",
    "- **False Positive (FP)**: Prediction is positive, but actual value is negative.\n",
    "- **False Negative (FN)**: Prediction is negative, but actual value is positive.\n",
    "- **True Negative (TN)**: Prediction and actual value are both negative.\n",
    "\n",
    "#### Relationships in the Confusion Matrix\n",
    "\n",
    "The values in the confusion matrix are related as follows:\n",
    "\n",
    "- **Prediction Outcome Positive**: $ TP + FP $\n",
    "- **Prediction Outcome Negative**: $ TN + FN $\n",
    "- **Condition Positive**: $ TP + FN $\n",
    "- **Condition Negative**: $ TN + FP $\n",
    "- **Total Population**: $ TP + FP + TN + FN $\n",
    "\n",
    "#### Performance Metrics\n",
    "\n",
    "**1. Accuracy**\n",
    "- **Formula**: $\\frac{TP + TN}{TP + FP + TN + FN}$\n",
    "- **Intuition**: Measures the overall correctness of the model. However, it may not be reliable if the classes are imbalanced.\n",
    "\n",
    "**2. True Positive Rate (TPR) / Sensitivity / Recall**\n",
    "- **Formula**: $\\frac{TP}{TP + FN}$\n",
    "- **Intuition**: Measures the percentage of actual positives correctly identified by the model.\n",
    "\n",
    "**3. False Negative Rate (FNR)**\n",
    "- **Formula**: $\\frac{FN}{TP + FN}$\n",
    "- **Intuition**: Measures the percentage of actual positives that are missed by the model. It is $1 - TPR$.\n",
    "\n",
    "**4. False Positive Rate (FPR)**\n",
    "- **Formula**: $\\frac{FP}{FP + TN}$\n",
    "- **Intuition**: Measures the percentage of actual negatives incorrectly identified as positives. It is $1 - \\text{True Negative Rate}$.\n",
    "\n",
    "**5. True Negative Rate (TNR) / Specificity**\n",
    "- **Formula**: $\\frac{TN}{FP + TN}$\n",
    "- **Intuition**: Measures the percentage of actual negatives correctly identified by the model.\n",
    "\n",
    "**6. Positive Predictive Value (PPV) / Precision**\n",
    "- **Formula**: $\\frac{TP}{TP + FP}$\n",
    "- **Intuition**: Measures the percentage of predicted positives that are actually positive.\n",
    "\n",
    "**7. False Discovery Rate (FDR)**\n",
    "- **Formula**: $\\frac{FP}{TP + FP}$\n",
    "- **Intuition**: Measures the percentage of predicted positives that are actually negative. It is $1 - PPV$.\n",
    "\n",
    "**8. Negative Predictive Value (NPV)**\n",
    "- **Formula**: $\\frac{TN}{TN + FN}$\n",
    "- **Intuition**: Measures the percentage of predicted negatives that are actually negative.\n",
    "\n",
    "**9. False Omission Rate (FOR)**\n",
    "- **Formula**: $\\frac{FN}{TN + FN}$\n",
    "- **Intuition**: Measures the percentage of predicted negatives that are actually positive. It is $1 - NPV$.\n",
    "\n",
    "**10. Prevalence**\n",
    "- **Formula**: $\\frac{TP + FN}{TP + FP + TN + FN}$\n",
    "- **Intuition**: Measures the proportion of actual positives in the population.\n",
    "\n",
    "**11. F1 Score**\n",
    "- **Formula**: $\\frac{2 \\times PPV \\times TPR}{PPV + TPR}$\n",
    "- **Intuition**: Combines precision and recall into a single metric by calculating their harmonic mean. Useful when you need a balance between precision and recall.\n",
    "\n",
    "#### Example Calculation\n",
    "\n",
    "Given:\n",
    "- **True Positives (TP)**: 55\n",
    "- **False Positives (FP)**: 100\n",
    "- **False Negatives (FN)**: 10\n",
    "- **True Negatives (TN)**: 835\n",
    "\n",
    "Let's calculate:\n",
    "\n",
    "- **Accuracy**: $\\frac{55 + 835}{1000} = 0.89$ or 89%\n",
    "- **True Positive Rate (Recall)**: $\\frac{55}{55 + 10} = 0.85$ or 85%\n",
    "- **False Negative Rate**: $1 - 0.85 = 0.15$ or 15%\n",
    "- **False Positive Rate**: $\\frac{100}{100 + 835} = 0.11$ or 11%\n",
    "- **True Negative Rate (Specificity)**: $\\frac{835}{1000 - 55 - 10} = 0.89$ or 89%\n",
    "- **Positive Predictive Value (Precision)**: $\\frac{55}{55 + 100} = 0.35$ or 35%\n",
    "- **Negative Predictive Value**: $\\frac{835}{835 + 10} = 0.99$ or 99%\n",
    "- **False Discovery Rate**: $1 - 0.35 = 0.65$ or 65%\n",
    "- **False Omission Rate**: $1 - 0.99 = 0.01$ or 1%\n",
    "- **F1 Score**: $\\frac{2 \\times 0.35 \\times 0.85}{0.35 + 0.85} \\approx 0.50$\n",
    "\n",
    "#### Choosing the Best Model\n",
    "\n",
    "When comparing models, you might see variations in metrics across different models. For instance:\n",
    "\n",
    "- **Model A**: Accuracy = 0.68, PPV = 0.65, F1 = 0.66\n",
    "- **Model B**: Accuracy = 0.50, PPV = 0.50, F1 = 0.61\n",
    "- **Model C**: Accuracy = 0.82, PPV = 0.86, F1 = 0.81\n",
    "\n",
    "**Model C** appears best as it has higher performance across accuracy, PPV, and F1 score. However, the choice might also depend on the specific application and the cost of false positives and false negatives.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Confusion Matrix** is essential for understanding model performance.\n",
    "- **Performance Metrics** such as accuracy, precision, recall, F1 score, etc., provide insights into different aspects of model performance.\n",
    "- **Choosing Metrics** depends on the context and requirements of the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "The concept of threshold in binary classification and the ROC (Receiver Operating Characteristic) curve is crucial for understanding model performance. Here’s a breakdown to help clarify these concepts and how to use them effectively:\n",
    "\n",
    "### 1. **Understanding Thresholds**\n",
    "\n",
    "- **Threshold**: In binary classification, a threshold is a cut-off value used to decide whether a predicted probability should be classified as positive or negative. For instance, if your model outputs a probability score between 0 and 1, a threshold of 0.5 means that any score above 0.5 is classified as positive, while scores below 0.5 are classified as negative.\n",
    "\n",
    "- **Impact of Threshold**: Changing the threshold affects both the True Positive Rate (TPR) and the False Positive Rate (FPR). A higher threshold means fewer positives are predicted (which could lead to more False Negatives), and a lower threshold means more positives are predicted (which could lead to more False Positives).\n",
    "\n",
    "### 2. **ROC Curve**\n",
    "\n",
    "- **ROC Curve**: The ROC curve is a graphical representation of the performance of a classification model across all possible thresholds. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) for different threshold values.\n",
    "\n",
    "  - **True Positive Rate (TPR)**: Also known as Sensitivity or Recall, it measures the proportion of actual positives correctly identified by the model.\n",
    "  \n",
    "  - **False Positive Rate (FPR)**: It measures the proportion of actual negatives that are incorrectly classified as positive by the model.\n",
    "\n",
    "- **Creating the ROC Curve**: To create the ROC curve, you:\n",
    "  1. Sort the prediction scores in descending order.\n",
    "  2. Use each score as a threshold to classify predictions.\n",
    "  3. Calculate TPR and FPR for each threshold.\n",
    "  4. Plot these values to form the ROC curve.\n",
    "\n",
    "### 3. **Area Under the ROC Curve (AUC)**\n",
    "\n",
    "- **AUC**: The Area Under the ROC Curve (AUC) quantifies the overall ability of the model to discriminate between positive and negative classes. An AUC of 1 indicates perfect classification, while an AUC of 0.5 indicates no discrimination (similar to random guessing).\n",
    "\n",
    "- **Why AUC is Useful**: AUC is a single value that summarizes the performance of the model across all thresholds, making it a useful metric when comparing different models.\n",
    "\n",
    "### 4. **Choosing the Best Threshold**\n",
    "\n",
    "- **Selecting Thresholds**: The choice of the threshold depends on the trade-offs between True Positives and False Positives. The optimal threshold can vary based on:\n",
    "  - **Objective**: If minimizing False Positives is crucial (e.g., in spam detection where false positives are costly), you might choose a higher threshold.\n",
    "  - **Objective**: If maximizing True Positives is more important (e.g., in medical diagnoses where missing a positive case could be severe), you might choose a lower threshold.\n",
    "\n",
    "- **Optimum Point**: The ideal threshold is often one that lies close to the top left corner of the ROC curve, where the TPR is high and the FPR is low. This point represents the best balance between sensitivity and specificity.\n",
    "\n",
    "### 5. **Quiz Example**\n",
    "\n",
    "When faced with options A, B, C, and D for threshold values:\n",
    "\n",
    "- **Evaluate based on your priorities**:\n",
    "  - **If you care about minimizing False Positives**: Choose a threshold that yields a lower FPR.\n",
    "  - **If you care about maximizing True Positives**: Choose a threshold that yields a higher TPR.\n",
    "  - **If you want a balance**: Choose a threshold that provides a good trade-off between TPR and FPR.\n",
    "\n",
    "- **Practical Decision**: In practice, you often choose a threshold based on the ROC curve analysis and the specific requirements of your application. For instance, if your ROC curve is available, selecting a point along the curve that balances TPR and FPR according to your needs is ideal.\n",
    "\n",
    "Understanding these concepts will help you better assess and optimize the performance of your classification models, ensuring they meet the specific needs and constraints of your problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Clustering Metrics\n",
    "\n",
    "Here’s a summary of popular regression and clustering performance metrics, along with some insights into their uses and limitations:\n",
    "\n",
    "### **Regression Metrics**\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**\n",
    "   - **Definition**: MAE is the average of the absolute differences between predicted and actual values.\n",
    "   - **Formula**: $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$\n",
    "   - **Pros**: \n",
    "     - Intuitive and easy to understand.\n",
    "     - Robust to outliers, as it does not square the errors.\n",
    "   - **Cons**:\n",
    "     - Not differentiable at zero, which can be a drawback for optimization algorithms.\n",
    "     - Not bounded, so comparisons across datasets can be challenging.\n",
    "\n",
    "2. **Mean Squared Error (MSE)**\n",
    "   - **Definition**: MSE is the average of the squares of the differences between predicted and actual values.\n",
    "   - **Formula**: $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$\n",
    "   - **Pros**:\n",
    "     - Differentiable, which makes it easier to optimize (e.g., using gradient descent).\n",
    "     - Penalizes larger errors more heavily due to squaring.\n",
    "   - **Cons**:\n",
    "     - Sensitive to outliers because it squares the errors.\n",
    "     - Not bounded, making cross-dataset comparisons difficult.\n",
    "\n",
    "3. **R-Squared ($R^2$)**\n",
    "   - **Definition**: $R^2$ is the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "   - **Formula**: $R^2 = 1 - \\frac{\\text{MSE}}{\\text{Variance of } y}$\n",
    "   - **Pros**:\n",
    "     - Provides a normalized measure of how well the model fits the data.\n",
    "     - Ranges from 0 to 1 (or can be negative if the model performs worse than a simple mean).\n",
    "   - **Cons**:\n",
    "     - Can be misleading in some contexts, such as when comparing models of different complexities.\n",
    "     - Not always a good measure if the data has a complex structure that isn’t well-captured by the model.\n",
    "\n",
    "### **Clustering Metrics**\n",
    "\n",
    "1. **Rand Index**\n",
    "   - **Definition**: Measures the similarity between two data clusterings by comparing the number of pairs of points that are clustered together or apart in both clusterings.\n",
    "   - **Formula**: $\\text{Rand Index} = \\frac{a + b}{\\binom{n}{2}}$, where $a$ is the number of pairs in the same cluster in both assignments and $b$ is the number of pairs in different clusters in both assignments.\n",
    "   - **Pros**:\n",
    "     - Bounded between 0 and 1.\n",
    "     - Accounts for both agreements and disagreements between clustering assignments.\n",
    "   - **Cons**:\n",
    "     - Requires ground truth clustering, which may not always be available.\n",
    "\n",
    "2. **Mutual Information**\n",
    "   - **Definition**: Measures the amount of information obtained about one clustering from the other.\n",
    "   - **Formula**: $\\text{MI}(X, Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)}$\n",
    "   - **Normalized Mutual Information (NMI)**: $\\text{NMI} = \\frac{\\text{MI}(X, Y)}{\\sqrt{\\text{Entropy}(X) \\times \\text{Entropy}(Y)}}$\n",
    "   - **Pros**:\n",
    "     - Provides a measure of the dependency between two clustering assignments.\n",
    "     - Bounded between 0 and 1.\n",
    "   - **Cons**:\n",
    "     - Requires ground truth clustering.\n",
    "     - Can be complex to compute and interpret.\n",
    "\n",
    "3. **Silhouette Coefficient**\n",
    "   - **Definition**: Measures how similar a data point is to its own cluster compared to other clusters. It considers both cohesion (how close points are within the same cluster) and separation (how far points are from other clusters).\n",
    "   - **Formula**: $s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$, where $a(i)$ is the average distance from point $i$ to other points in the same cluster, and $b(i)$ is the average distance from point $i$ to points in the nearest cluster.\n",
    "   - **Pros**:\n",
    "     - Does not require ground truth clustering.\n",
    "     - Bounded between -1 and 1; higher values indicate better clustering.\n",
    "   - **Cons**:\n",
    "     - Assumes spherical clusters, so it may not work well for non-convex shapes.\n",
    "     - Can be sensitive to the scale of the data.\n",
    "\n",
    "### **Choosing the Right Metric**\n",
    "\n",
    "- **Regression**: Choose MAE if robustness to outliers is important, MSE if you need a differentiable metric for optimization, or $R^2$ if you want a normalized measure of fit.\n",
    "\n",
    "- **Clustering**: Use Rand Index and Mutual Information if you have ground truth clusterings and want measures of agreement, and use Silhouette Coefficient if you want a metric that does not require ground truth but be aware of its assumption about cluster shapes.\n",
    "\n",
    "Understanding these metrics will help you evaluate and improve your models effectively, tailoring your approach to the specific characteristics and requirements of your data and problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
